# 1th part
## 单层感知机与多层感知机
>单层感知机：线性空间；多层感知机：非线性空间  
>多层感知机=多层结构的神经网络：第0层输入，第1，2...n-1层处理，第n层输出  
## 感知机激活函数
>初始公式x1w1+x2w2+(-theta)<=0输出0  
>x1w1+x2w2+(-theta)> 0输出1（w是输入信号的权重）  
>btheta（b是偏置，激活神经元的阈值)  
简化:将输出阈值也作为一个信号的权重，保持该信号为1即可。引入激活函数h(a)(当输入大于0输出1，小于等于0输出0阶跃函数）  
>a=b+Σxiwi  
>y=h(a)
## 本地导入和mnist.py脚本的运行
1.http://yann.lecun.com/exdb/mnist/  
> 下载相关文件后（.gz）放入书中（深度学习入门）源代码的dataset目录下，此时运行mnist.py脚本文件则成功  

2.在自己写的deeplearn.py文件中引用load_mnist文件  
> 同层文件目录`import mnist.py`  
> load_mnist.py在deeplearn.py所在文件夹的子文件夹dataset内
>> 1)`import dataset.test`
`print("in deeplearn.py import:")`
`dataset.test.test_fileimport()`  
>> 2)`from dataset.test import test_fileimport``test_fileimport()`这种可以直接用导入的函数但是容易报错，最好在导入文件的文件夹里新建一个_init_.py  
>> 3)`import sys,os`
`sys.path.append(os.pardir)#将父目录加入搜索路径`
`from dataset.test import test_fileimport`
`test_fileimport()`
## 神经网络学习过程小结
> 挑一部分数据——进行处理并对输出用softmax进行概率化后与监督数据t进行对比计算损失函数与损失函数对权重的梯度——将权重参数沿着梯度方向进行更新——重复  
> 1.输入数据(采用了minibatch办法，对输入数据正规化操作)->权重加偏置进行计算->激活函数进行整理->输出下一层的值  
> 2.输出层结果与标签计算损失函数->根据损失函数计算其相对权重的梯度->对权重进行对应的变大/变小操作，变化幅度对应梯度大小  
> 3.用得到的新权重重复1，2操作  
# 2th part
## 基础知识
1. 计算机学习的定义：经验E+任务T+性能度量P  
2. 监督学习和非监督学习（有无答案）  
> 监督学习：回归——预测连续值的属性/输出；分类——预测离散值的属性/输出）  
> 无监督学习：聚类算法
3. 寻找合适的代价函数  
> 代价函数：假设模型函数h的输出hi(x)与实际输出y的差别度量函数J(i:函数h的变量)=损失函数J，也可以称为模型与数据的拟合差  
> 线代补充：A*·A=|A|·E；A-1=A*/E  
> 特征缩放与均值平均化：x=x-u(x的平均值)  
> 学习率：过小学得慢过大跑太快错过最小值点会使得损失函数变成增函数  
> 正规方程：直接计算权重的最优值而非梯度下降调整获得其最优值—W=(np.reverse(np.dot(X.T,X)))*y；计算复杂，大规模运行很慢
